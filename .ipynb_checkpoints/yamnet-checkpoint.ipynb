{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e9e1b95a-881b-49a2-9a0d-671cc820ac97",
   "metadata": {},
   "outputs": [],
   "source": [
    "import librosa\n",
    "import numpy as np\n",
    "from pathlib import Path\n",
    "import tensorflow as tf\n",
    "import tensorflow_hub as hub"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d513cef0-f351-49bd-835e-837ae9c29ca1",
   "metadata": {},
   "source": [
    "## Preprocessing steps\n",
    "* ### Load the data from raw audio files\n",
    "* ### Max duration is 5 second\n",
    "* ### Sampling rate of 16K Hz\n",
    "* ### Create separate csv files for Cry, Scream and Normal voice"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf78c650-75c1-4d10-ac8c-f0e745060f25",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 289,
   "id": "4c50abb1-7740-4fbc-a0cc-630c4e26a9e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# def extract_features(basepath, category, sr=16000, duration=5):\n",
    "#     \"\"\"\n",
    "#     Convert audio files to numpy array.\n",
    "#     Arg:\n",
    "#         basepath(pathlib): location for datasets\n",
    "#         category(str): 'cry', 'scream', 'normal'\n",
    "#         sr (int): sampling rate\n",
    "#         duration(float): duration in seconds\n",
    "#     Remarks:\n",
    "#         Create a numpy array of size (N, timestep_samples)\n",
    "#         Saves <category>.csv in the basepath. \n",
    "#     \"\"\"\n",
    "#     cat_path = basepath/category\n",
    "#     data = list()\n",
    "#     for file in cat_path.iterdir():\n",
    "#         y, _ = librosa.load(file, sr=sr, duration=duration)\n",
    "#         length = sr*duration\n",
    "#         if y.shape[0] != length:\n",
    "#             y = np.pad(y, pad_width=(0,length - y.shape[0]),mode='constant', constant_values = (0,0))\n",
    "#         data.append(y)\n",
    "#     data = np.array(data)\n",
    "#     np.savetxt(basepath/f'{category}.csv', data, delimiter=',')\n",
    "#     print(f\"File {category}.csv saved successfully\")\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 291,
   "id": "6c72cab5-2e13-4fce-ab74-805faf1006cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# extract_features(basepath, 'cry')\n",
    "# extract_features(basepath, 'scream')\n",
    "# extract_features(basepath, 'normal')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "03fd4a61-cbc7-475d-8484-a7852272125a",
   "metadata": {},
   "outputs": [],
   "source": [
    "basepath = Path.cwd()/'datasets'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "e8ec3770-f06c-4b9f-92c3-6648cdb854b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compile_filenames(basepath, category_dict):\n",
    "    \"\"\"\n",
    "    Make list of tuple (filename, category) in basepath\n",
    "    Args:\n",
    "        basepath(path): path of the dataset\n",
    "        categorydict(dict): key - category, value - class label\n",
    "    Returns:\n",
    "        list[tuple]: list of tuple (filename, classlabel)\n",
    "    \"\"\"\n",
    "    filenames = list()\n",
    "    labels = list()\n",
    "    for cat in category_dict.keys():\n",
    "        path = basepath/cat\n",
    "        label = category_dict[cat]\n",
    "        for file in path.iterdir():\n",
    "            filenames.append(str(file))\n",
    "            labels.append(label)\n",
    "    return filenames, labels\n",
    "\n",
    "def audio_to_numpy(filename, duration=5, sr = 16000):\n",
    "    \"\"\"\n",
    "    \"\"\"\n",
    "    filename = filename.numpy().decode('utf-8')\n",
    "    y, _ = librosa.load(filename, sr=sr, duration=duration)\n",
    "    length = sr*duration\n",
    "    if y.shape[0] != length:\n",
    "        y = np.pad(y, pad_width=(0,length - y.shape[0]),mode='constant', constant_values = (0,0))\n",
    "    return y\n",
    "\n",
    "def convert_audio(filename, label):\n",
    "    \"\"\"\n",
    "    Reads file from disk and convert to time series data.\n",
    "    Args:\n",
    "        filename(path): path of audio file\n",
    "        label(int): class label of audio file\n",
    "    Returns:\n",
    "        np.ndarray: numpy array of 1D having values (0,1)\n",
    "    Remarks:\n",
    "        The sr is 16000 and time 5 second max\n",
    "        The shape of the output is (80000,)\n",
    "        Zero padding will be done if audio is not 5 sec.\n",
    "    \"\"\"\n",
    "    y = tf.py_function(audio_to_numpy, [filename], tf.float32)\n",
    "    return y, label\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "a614d6a0-9f5a-47e1-b2ee-25be8a39a432",
   "metadata": {},
   "outputs": [],
   "source": [
    "category_dict = {'normal':0, 'cry':1, 'scream':2}\n",
    "filenames, labels = compile_filenames(basepath, category_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "557c2262-76d2-4325-80e9-49b8eb0326f0",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I0000 00:00:1740336683.286265   62456 gpu_device.cc:2022] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 6982 MB memory:  -> device: 0, name: NVIDIA GeForce GTX 1070, pci bus id: 0000:01:00.0, compute capability: 6.1\n"
     ]
    }
   ],
   "source": [
    "dataset = tf.data.Dataset.from_tensor_slices((filenames, labels))\n",
    "dataset = dataset.map(convert_audio)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "0fc5d5b4-d8ed-4df1-8122-8dabb8781288",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_size = int(len(filenames)*0.7)\n",
    "val_size = int(len(filenames)*0.15)\n",
    "test_size = len(filenames) - train_size - val_size\n",
    "train_data = dataset.take(train_size)\n",
    "val_data = dataset.skip(train_size).take(val_size)\n",
    "test_data = dataset.skip(train_size + val_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b302355c-5ff8-49ed-9a54-4cac13fdb881",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "39035aed-441e-4b0d-82bc-b2a936f7d57a",
   "metadata": {},
   "source": [
    "## Finetune YamNet model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8fbc52a5-f4fe-4ef1-ad7f-2045eaa4e88d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "id": "53ff9ff1-1610-4c5a-bf7b-2f520fe35ffd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(None, 1024)\n"
     ]
    }
   ],
   "source": [
    "# Model Building\n",
    "class YamNetLayer(tf.keras.Layer):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        yamnet_model_handle = 'https://www.kaggle.com/models/google/yamnet/TensorFlow2/yamnet/1'\n",
    "        self.yamnet_model = hub.KerasLayer(yamnet_model_handle, trainable=False)\n",
    "    def call(self, x):\n",
    "        y = self.yamnet_model(x)[1]\n",
    "        print(y.shape)\n",
    "        return y\n",
    "\n",
    "audio_ip = tf.keras.Input(shape=())\n",
    "embeddings = YamNetLayer()(audio_ip)\n",
    "y = tf.keras.layers.Dense(512, activation='relu')(embeddings)\n",
    "y = tf.keras.layers.Dense(3, activation='softmax')(y)\n",
    "model = tf.keras.Model(inputs = audio_ip, outputs = y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "id": "97974fd6-6088-40db-8ffc-51d91c8a3eb8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"functional_9\"</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1mModel: \"functional_9\"\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
       "┃<span style=\"font-weight: bold\"> Layer (type)                    </span>┃<span style=\"font-weight: bold\"> Output Shape           </span>┃<span style=\"font-weight: bold\">       Param # </span>┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
       "│ input_layer_20 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">InputLayer</span>)     │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>)                 │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ yam_net_layer_14 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">YamNetLayer</span>)  │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1024</span>)           │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_15 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">512</span>)            │       <span style=\"color: #00af00; text-decoration-color: #00af00\">524,800</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_16 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">3</span>)              │         <span style=\"color: #00af00; text-decoration-color: #00af00\">1,539</span> │\n",
       "└─────────────────────────────────┴────────────────────────┴───────────────┘\n",
       "</pre>\n"
      ],
      "text/plain": [
       "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
       "┃\u001b[1m \u001b[0m\u001b[1mLayer (type)                   \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mOutput Shape          \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m      Param #\u001b[0m\u001b[1m \u001b[0m┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
       "│ input_layer_20 (\u001b[38;5;33mInputLayer\u001b[0m)     │ (\u001b[38;5;45mNone\u001b[0m)                 │             \u001b[38;5;34m0\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ yam_net_layer_14 (\u001b[38;5;33mYamNetLayer\u001b[0m)  │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m1024\u001b[0m)           │             \u001b[38;5;34m0\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_15 (\u001b[38;5;33mDense\u001b[0m)                │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m512\u001b[0m)            │       \u001b[38;5;34m524,800\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_16 (\u001b[38;5;33mDense\u001b[0m)                │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m3\u001b[0m)              │         \u001b[38;5;34m1,539\u001b[0m │\n",
       "└─────────────────────────────────┴────────────────────────┴───────────────┘\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">526,339</span> (2.01 MB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Total params: \u001b[0m\u001b[38;5;34m526,339\u001b[0m (2.01 MB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">526,339</span> (2.01 MB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m526,339\u001b[0m (2.01 MB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61a4e02f-5c0e-4cde-8e40-36d1299e7911",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3242123-9d65-4196-a73c-05d1f2176c68",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "id": "04875801-dcd4-4425-bd4e-bc296fcb36dc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(2, 1024)\n",
      "tf.Tensor(\n",
      "[[0.40163845 0.21706104 0.3813005 ]\n",
      " [0.3763554  0.16822122 0.45542333]], shape=(2, 3), dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "with tf.device(\"/cpu:0\"):\n",
    "    # for X,y in train_data:\n",
    "    #     y = model(X)\n",
    "    #     print(y.shape)\n",
    "    ip = np.random.random(size=(16000,))\n",
    "    y = model(ip)\n",
    "    print(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "26b6787b-6a54-44b2-8283-5e74ab5d34f6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "913679f5-99c9-4fab-9c5f-01eb9ccddf0f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c32dca0-8ff5-40b0-932f-5c60f132bae9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7fbe1ab2-cf48-45a2-89cf-2c0e83f791bc",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c27e724b-fac4-4db8-a3d1-5b1dfb35f98c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e851560b-0e22-4ec2-8876-8409a09d781d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8f86be5-174a-45eb-9540-079e48c54b8c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:base] *",
   "language": "python",
   "name": "conda-base-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
